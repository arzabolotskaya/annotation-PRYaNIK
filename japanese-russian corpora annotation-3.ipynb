{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Открываем файлы предыдущей разметки\n",
    "Преобразуем в формат, удобный для анализа структуры и поиска тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openfile(name):\n",
    "    with open(name, encoding='utf-8') as f:\n",
    "        file = f.read()\n",
    "        \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "файл 1 -- файл 2 -- файл 3 -- файл 4 -- файл 5 -- файл 6 -- файл 7 -- файл 8 -- файл 9 -- файл 10 -- файл 11 -- файл 12 -- файл 13 -- файл 14 -- файл 15 -- файл 16 -- файл 17 -- файл 18 -- файл 19 -- файл 20 -- файл 21 -- файл 22 -- файл 23 -- файл 24 -- файл 25 -- файл 26 -- файл 27 -- файл 28 -- файл 29 -- файл 30 -- файл 31 -- файл 32 -- файл 33 -- файл 34 -- файл 35 -- файл 36 -- файл 37 -- файл 38 -- файл 39 -- файл 40 -- файл 41 -- файл 42 -- файл 43 -- файл 44 -- файл 45 -- файл 46 -- файл 47 -- файл 48 -- файл 49 -- "
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for i in range(1, 50):\n",
    "    print(\"файл\", i, end=\" -- \")\n",
    "    name = \"/Users/alinazabolotskaya/Desktop/annotated/\" + str(i)+\".xml\"\n",
    "    try:\n",
    "        text = openfile(name)\n",
    "        files.append(text)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = ' '.join(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/alinazabolotskaya/Desktop/annotated/oldxmls1.xml\", \"w\") as text_file:\n",
    "    text_file.write(\"<texts>\")\n",
    "    text_file.write(joined)\n",
    "    text_file.write(\"</texts>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим вариант текстов только с японской разметкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<entry>\n",
      "    <id>1</id>\n",
      "    <url>\n",
      "        \n",
      "        <japanese>https://www.hokkoku.co.jp/subpage/HT20200205401.htm</japanese>\n",
      "    </url>\n",
      "    <title>\n",
      "        \n",
      "        <japanese>ロシア捕虜のベッドか　日露戦争時、金沢で収容　白山市で\n"
     ]
    }
   ],
   "source": [
    "joined = re.sub(r'<russian>[\\s\\S]*?<\\/russian>', '', joined, re.DOTALL)\n",
    "print(joined[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск тегов в существующем XML\n",
    "С помощью regex можно найти существующие теги и посмотреть, как их размечает JUMAN++. Например, личное местоимение あなた (\"ты\") размечается как Noun (\"существительное\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<name>pos</name>\\n                                <value>Noun</value>\\n                            </attribute>\\n                            <attribute>\\n                                <name>category</name>\\n                                <value>['People']</value>\\n                            </attribute>\\n                            <attribute>\\n                                <name>romaji_reading</name>\\n                                <value>anata</value>\\n                            </attribute>\\n                            <attribute>\\n                                <name>lexeme_reading</name>\\n                                <value>あなた</value>\"]"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.findall(r'<name>pos</name>[^,]*?<value>あなた</value>', joined, re.DOTALL)#[\\s\\S]*?\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<name>pos</name>\n",
      "                                <value>Noun</value>\n",
      "                            </attribute>\n",
      "                            <attribute>\n",
      "                                <name>category</name>\n",
      "                                <value>['People']</value>\n",
      "                            </attribute>\n",
      "                            <attribute>\n",
      "                                <name>romaji_reading</name>\n",
      "                                <value>anata</value>\n",
      "                            </attribute>\n",
      "                            <attribute>\n",
      "                                <name>lexeme_reading</name>\n",
      "                                <value>あなた</value>\n"
     ]
    }
   ],
   "source": [
    "for i in result[0].split(\"\\n\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Чтение XML файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(name):\n",
    "    \n",
    "    tree = ET.parse(name)\n",
    "    root = tree.getroot()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переразметка токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее из XML файла вытаскивается нужная информация и происходит переразметка с суффиксами и префиксами\n",
    "\n",
    "#Алгоритм таков:\n",
    "#    1) Достаем айди предложения, его перевод на русский и всю информацию о токене\n",
    "#    2) Создаем пустой список для записи морфем, которые должны объединиться в слово\n",
    "#    3) Если список морфемами слова пустой, \n",
    "#        а) добавляем текущий токен в список морфем, \n",
    "#        б) часть речи токена записываем в переменную previous - предыдущая морфема\n",
    "#    4) Если в списке что-то уже есть, проверяем, началось ли новое слово!\n",
    "#        а) если текущая морфема не суффикс, и предыдущая не приставка:\n",
    "#                    -> началось новое слово: -список морфем добавляется к списку слов, \n",
    "#                                             -список морфем снова становится пустым\n",
    "#        в) если новое слово не началось, повторяем пункты 3а) и 3б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure(root):\n",
    "    data = [] #список, в который мы будем вытаскивать нужную информацию о предложении: \n",
    "                        #айди, предложение на русском и японскую разметку в виде словаря\n",
    "\n",
    "    for child in root[3]:\n",
    "        s_id = child[0].text\n",
    "        s_rus = child[1][0].text\n",
    "        wordlist = [] # сюда сложим [префиксы, основу и суффиксы] списком в виде объектов деревьев и отправим в функцию на аннотацию\n",
    "        morphlist = [] # здесь соберем слово\n",
    "        for token in child[2][1]:\n",
    "            if morphlist == []:\n",
    "                try:\n",
    "                    for attr in token[5]:\n",
    "                        if attr[0].text == 'pos':\n",
    "                            previous = attr[1].text\n",
    "                    morphlist.append(token)\n",
    "                    #print(len(morphlist))\n",
    "                except IndexError: #не морфемные токены, например, знаки пунктуации\n",
    "                        #for t in token:\n",
    "                        #print(t.tag,t.text)\n",
    "                    wordlist.append([token])\n",
    "            else:\n",
    "                has_a = 0\n",
    "                for t in token:\n",
    "                    if t.tag == 'attributes':\n",
    "                        has_a = 1\n",
    "                        for attr in t:\n",
    "                            if attr[0].text == 'pos':\n",
    "                                #print(attr[1].text)\n",
    "                                if attr[1].text != 'Suffix' and previous != 'Prefix':\n",
    "                                    #print(\"Новое слово\")\n",
    "                                    wordlist.append(morphlist)\n",
    "                                    morphlist = []\n",
    "                                else:\n",
    "                                    pass#print(attr[1].text, previous)\n",
    "                                previous = attr[1].text\n",
    "                        morphlist.append(token)\n",
    "                        #print(len(morphlist))\n",
    "                if has_a == 0:\n",
    "                        #wordlist.append(morphlist)\n",
    "                    wordlist.append([token])\n",
    "                    morphlist = []\n",
    "\n",
    "                \n",
    "        data.append((s_id, s_rus, wordlist))\n",
    "    c = 0\n",
    "    for i in data:\n",
    "        for j in i[2]:\n",
    "            if len(j) > 1:\n",
    "                c += 1\n",
    "    print(\"Количество переразмеченных слов в этом тексте:\", c)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Промежуточный формат, удобный для аннотирования\n",
    "#### Теперь запишем в словарь теги из объектов дерева и отправим токены на аннотацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(data):\n",
    "    all_data = []\n",
    "    for se in data:\n",
    "        full_pre = []\n",
    "        trouble = se[2]\n",
    "        pre_wordlist = []\n",
    "        for token in trouble:\n",
    "            pre_dict = {}\n",
    "            if len(token) == 1: #для токенов без аффиксов функция аннотации отлична от функции с аффиксами, так как требуется усложненный глоссинг \n",
    "                for attr in token[0]:\n",
    "                    if attr.tag not in [\"attributes\", \"extraAttributes\"]:\n",
    "                        if attr.tag not in pre_dict:\n",
    "                            pre_dict.update({attr.tag: attr.text})\n",
    "                        else:\n",
    "                            print(attr.tag, \"повторяется!\")\n",
    "                    elif attr.tag == \"attributes\":\n",
    "                        for a in attr:\n",
    "                            if a[0].text not in pre_dict:\n",
    "                                pre_dict.update({a[0].text: a[1].text})\n",
    "                            else:\n",
    "                                print(a[0].text, \"повторяется!\")\n",
    "\n",
    "                    elif attr.tag == \"extraAttributes\":\n",
    "                        extra = []\n",
    "                        for a in attr:\n",
    "                            extra.append(a.text)\n",
    "                        pre_dict.update({attr.tag: extra})\n",
    "\n",
    "                    else:\n",
    "                        print(\"!!!!unknown attrib\", attr.tag)\n",
    "                ann_dict = ana(pre_dict)\n",
    "                pre_wordlist.append(ann_dict)\n",
    "            else:\n",
    "                pass \n",
    "        full_pre.append(pre_wordlist)\n",
    "        ready = (se[0], se[1], full_pre)\n",
    "        all_data.append(ready)\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как мы видим, теги не повторяются! Поэтому можно использовать формат словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация разметки. Приписывание тегов. Создание слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Переразметка местоимений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron = \"You, あちこち, あちら, あなた, ある, あれ, いずこ, いずれ, いつ, おまえ, おめー, お前, か, かれ, ここ, こちら, こっち, これ, こんな, そこ, そちら, そらあ, それ, そんな, どこ, どちら, どなた, どれ, どんな, なに, なん, ぼく, わしゃ, わたし, われわれ, オタク, キミ, ボク, ヲタク, 何, 何処, 何時, 余, 俺, 僕, 君, 己, 彼, 彼女, 彼方, 我, 我々, 私, 誰, 貴様\".split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tr(text): #в дальнейшем возможно подключение mystem, nltk\n",
    "    text = re.sub('<i>|</i>|<b>|</b>|<br>|</br>', ' ', text)\n",
    "    text = re.sub('<span class=\"gtat\">', '(', text)\n",
    "    text = re.sub('</span>', ')', text)\n",
    "    if text.startswith(\"('\"): text = text[2:]\n",
    "    if text.endswith(\"',)\"): text = text[:-3]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana(dic):\n",
    "    text, gr, has_translit, gloss, lex, transl, sem, translit, hiragana, lex_translit, hiragana_wf, ortho, morphon = '','','','','','','','','','','','',''\n",
    "    text = dic[\"text\"]\n",
    "    if dic[\"text\"] == \"高岸寺\":\n",
    "        result = {\"text\": \"高岸寺\"}\n",
    "    else:\n",
    "        if text.isalpha() == True:\n",
    "            if 'lexeme' in dic:\n",
    "                lex = ''\n",
    "                lex = dic['lexeme']\n",
    "\n",
    "                \n",
    "        #ЧАСТИ РЕЧИ\n",
    "            gr_tags =[]              # -- CHANGES\n",
    "            if 'pos' in dic:\n",
    "                pos = ''\n",
    "                if dic['pos'] == 'Verb':\n",
    "                    if 'type' in dic and dic['type'] == 'Auxiliary':\n",
    "                        pos = 'AUX'\n",
    "                    else: pos = 'V'\n",
    "\n",
    "                elif dic['pos'] == 'Adjective':\n",
    "                    if 'type' in dic:\n",
    "                        if dic['type'] == 'Na': pos = 'A,ADNOM1'\n",
    "                        elif dic['type'] == 'I': pos = 'A,ADPRAEDIC'\n",
    "                    else: pos = 'A,ADNOM2' \n",
    "                elif dic['pos'] == 'Noun':\n",
    "                    if dic['text'] in pron:\n",
    "                        pos = 'PRO'\n",
    "                    else: pos = 'S'\n",
    "                else: pos = pos_map[dic['pos']]\n",
    "                if pos:\n",
    "                    gr_tags.append(pos)              # -- CHANGES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #ГЛАГОЛЬНОЕ СЛОВОИЗМЕНЕНИЕ\n",
    "                \n",
    "                if pos == 'V':\n",
    "                    if 'row' in dic: gr_tags.append(dic['row'][0].lower())\n",
    "                    if 'stem_type' in dic: gr_tags.append(verb_map[dic['stem_type']])\n",
    "                    else: gr_tags.append(\"3\")\n",
    "                    if 'form1' in dic: gr_tags.append(verb_map[dic['form1']])\n",
    "                    if 'transitivity' in dic: gr_tags.append(verb_map[dic['transitivity']])\n",
    "                    if 'form2' in dic: gr_tags.append(verb_map[dic['form2']])\n",
    "                        \n",
    "        #СЛОВОИЗМЕНЕНИЕ ПРИЛАГАТЕЛЬНЫХ \n",
    "                if pos in [\"A,ADPRAEDIC\",\"A,ADNOM1\",\"A,ADNOM2\"]:\n",
    "                    if 'form1' in dic: gr_tags.append(adj_map[dic['form1']])\n",
    "                    if 'transitivity' in dic: gr_tags.append(adj_map[dic['transitivity']])\n",
    "                    if 'form2' in dic: gr_tags.append(adj_map[dic['form2']])\n",
    "\n",
    "                gr = \",\".join(gr_tags)\n",
    "                \n",
    "                \n",
    "        #ТЕГИ ЧАСТИЦ\n",
    "                if pos == 'PART':\n",
    "                    if 'type' in dic and dic['type'] == 'Case_marking': \n",
    "                        gr_tags.append(\"case\")\n",
    "                        if dic['romaji_reading'] == \"ga\": gr_tags.append(\"nom\")\n",
    "                        elif dic['romaji_reading'] == \"no\": gr_tags.append(\"gen\")\n",
    "                        elif dic['romaji_reading'] == \"ni\": gr_tags.append(\"dat\")\n",
    "                        elif dic['romaji_reading'] == \"wo\": gr_tags.append(\"acc\")\n",
    "                        elif dic['romaji_reading'] == \"e\": gr_tags.append(\"lat\")\n",
    "                        elif dic['romaji_reading'] == \"kara\": gr_tags.append(\"abl\")\n",
    "                        elif dic['romaji_reading'] == \"de\": gr_tags.append(\"ins\")\n",
    "                        elif dic['romaji_reading'] == \"made\": gr_tags.append(\"ill\")\n",
    "                        elif dic['romaji_reading'] == \"yori\": gr_tags.append(\"eq\")\n",
    "                    elif 'type' in dic and dic['type'] == 'Adverbial': \n",
    "                        gr_tags.append(\"adv\")\n",
    "                        if 'romaji_reading' == \"wa\": gr_tags.append(\"top\")\n",
    "                        elif 'romaji_reading' == \"to\": gr_tags.append(\"com\")\n",
    "                    elif 'type' in dic and dic['type'] == 'Conjunctive':\n",
    "                        gr_tags.append(\"conj\")\n",
    "                    elif 'type' in dic and dic['type'] == 'Sentence_ending':\n",
    "                        gr_tags.append(\"sfin\")\n",
    "                    elif 'type' in dic and dic['type'] not in ['Conjunctive', 'Case_marking', 'Adverbial']:\n",
    "                        print(dic['type'])\n",
    "                gr = \",\".join(gr_tags)    \n",
    "                            \n",
    "\n",
    "        #СЛОЙ ПЕРЕВОДА\n",
    "                if 'translation' in dic and type(dic['translation']) == str: \n",
    "                    transl = clean_tr(dic['translation'])\n",
    "                    if transl == 'перевод отсутствует':      #-----CHANGES\n",
    "                        transl = ''\n",
    "                    elif transl.startswith('перевод неоднозначен'):\n",
    "                        transl = transl[20:]\n",
    "                    \n",
    "                    \n",
    "        #СЛОЙ СЕМАНТИКИ\n",
    "            sem = ''\n",
    "\n",
    "            if \"domain\" in dic:\n",
    "                domain = dic['domain'][2:-2].split(\"', '\")\n",
    "                \n",
    "                for d in domain:\n",
    "                    if sem != '':\n",
    "                        sem += ','\n",
    "                    sem = sem + sem_map[d]\n",
    "\n",
    "            if \"category\" in dic:\n",
    "                cat = dic['category'][2:-2].split(\"', '\")\n",
    "                \n",
    "                for c in cat:\n",
    "                    if sem != '':\n",
    "                        sem += ','\n",
    "                    sem = sem + sem_map[c]\n",
    "\n",
    "\n",
    "        #СЛОИ ТРАНСЛИТЕРАЦИИ         #-----CHANGES\n",
    "            translit, hiragana, lex_translit, hiragana_wf, has_translit = '', '', '', '', ''\n",
    "            if 'romanized_lexeme_reading' in dic: \n",
    "                lex_translit = clean_tr(dic['romanized_lexeme_reading'])\n",
    "                has_translit = 'true'\n",
    "            if 'lexeme_reading' in dic: hiragana = dic['lexeme_reading']\n",
    "            if 'romaji_reading' in dic: \n",
    "                translit = clean_tr(dic['romaji_reading'])\n",
    "                has_translit = 'true'\n",
    "            if 'hiragana_wf' in dic: hiragana_wf = dic['reading']\n",
    "          \n",
    "        \n",
    "        #ДРУГИЕ СЛОИ \n",
    "            if 'extraAttributes' in dic: \n",
    "                extras = dic['extraAttributes']\n",
    "                ortho = []\n",
    "                if \"Kanji\" in extras: ortho.append('kanji')\n",
    "                if \"Hiragana\" in extras: ortho.append('hiragana')\n",
    "                if \"Katakana\" in extras: ortho.append('katakana')\n",
    "                if \"Latin_letters\" in extras: ortho.append('romaji')\n",
    "                if \"Digits\" in extras: ortho.append('digit')\n",
    "                ortho = ','.join(ortho)\n",
    "\n",
    "                morphon = []\n",
    "                if \"Euphonic_change\" in extras: morphon.append('euphonic')\n",
    "                if \"Nasalization_change\" in extras: morphon.append('nasalisation')\n",
    "                morphon = ','.join(morphon)\n",
    "\n",
    "            result = {\n",
    "    \"text\": text,\n",
    "    #\"gloss\": gloss,\n",
    "    \"lex\": lex,\n",
    "    \"gr\": gr,\n",
    "    \"transl\": transl,\n",
    "    \"sem\": sem,\n",
    "    \"translit\": translit,                                                            #---- CHANGES\n",
    "    'has_translit': has_translit,\n",
    "    'lex_translit': lex_translit,      \n",
    "    \"hiragana\": hiragana,\n",
    "    \"hiragana_wf\": hiragana_wf,                                            #---- CHANGES\n",
    "    \"ortho\": ortho,\n",
    "    \"morphon\": morphon\n",
    "                  }\n",
    "        else:\n",
    "            result = {'text': dic['text'], 'punct': True}\n",
    "    return result\n",
    "    #else: return {'text': dic['text'], 'punct': True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.findall(r\"\"\"<value>Particle</value>\n",
    "                            </attribute>\n",
    "                            <attribute>\n",
    "                                <name>romaji_reading</name>\n",
    "                                <value>[^,]*?</value>\"\"\", joined, re.DOTALL)#[\\s\\S]*?\n",
    "ref = []\n",
    "for r in result:\n",
    "    if r not in ref:\n",
    "        ref.append(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Здесь описаны соответствия тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_map = {\n",
    "    \"Consonant\": \"1\",\n",
    "    \"Vowel\": \"2\",\n",
    "    \"Transitive\": \"tran\",\n",
    "    \"Intransitive\": \"intr\",\n",
    "    \"Ta\": \"pfv\",\n",
    "    \"Basic\": \"inf\",\n",
    "    \"Te\": \"cnv\",\n",
    "    \"Volitional\": \"vol\",\n",
    "    \"Assumptional\": \"hor\",\n",
    "    \"Conjunctive\": \"conj\",\n",
    "    \"Conditional\": \"cond\",\n",
    "    \"Imperative\": \"imp\",\n",
    "    \"Written\": \"wr\",\n",
    "    \"Imperfective\": \"ipf\",\n",
    "    \"Both\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_map = {\n",
    "    \"Ta\": \"pfv\",\n",
    "    \"Transitive\": \"tran\",\n",
    "    \"Intransitive\": \"intr\",\n",
    "    \"Basic\": \"inf\",\n",
    "    \"Te\": \"cnv\",\n",
    "    \"Volitional\": \"vol\",\n",
    "    \"Assumptional\": \"hor\",\n",
    "    \"Conjunctive\": \"conj\",\n",
    "    \"Conditional\": \"cond\",\n",
    "    \"Imperative\": \"imp\",\n",
    "    \"Written\": \"wr\",\n",
    "    \"Imperfective\": \"ipf\",\n",
    "    \"Both\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_map = {\n",
    "\"Culture and fine arts\": \"top:cult\",\n",
    "\"Recreation\": \"top:recrea\",\n",
    "\"Sports\": \"top:sports\",\n",
    "\"Health and medicine\": \"top:health\", \n",
    "\"Home and family life\": \"top:family\",\n",
    "\"Food and cooking\": \"top:cook\",\n",
    "\"Transportation\": \"top:transp\",\n",
    "\"Education and learning\": \"top:learn\",\n",
    "\"Science and art\": \"top:science\",\n",
    "\"Business\": \"top:business\",\n",
    "\"Media\": \"top:media\",\n",
    "\"Politics\": \"top:politics\",\n",
    "\"Unspecified\": \"top:unspec\",\n",
    "\"Organisations and associations\": \"t:orgz\",\n",
    "\"Animals\": \"t:animal\",\n",
    "\"Plants\": \"t:plant\",\n",
    "\"Animals-Parts\": \"pt:part\",\n",
    "\"Plants-Parts\": \"pt:part\",\n",
    "\"Crafted-Food\":\"t:tool:food\",\n",
    "\"Crafted-Clothes\":\"t:tool:cloth\",\n",
    "\"Crafted-Vehicles\":\"t:tool:transp\",\n",
    "\"Crafted-Money\":\"t:tool:money\",\n",
    "\"Crafted-Other\":\"t:tool:other\",\n",
    "\"Nature\":\"t:nature\",\n",
    "\"Places-Facilities\":\"t:place:facil\",\n",
    "\"Places-Facilities-Parts\":\"pt:part & pc:place:facil\",\n",
    "\"Places-Nature\":\"t:place:nature\",\n",
    "\"Places-Functional\":\"t:place:funct\",\n",
    "\"Places-Other\":\"t:place:other\",\n",
    "\"Abstract\":\"t:abstr\",\n",
    "\"Phenomena-Nature\":\"t:phenom:nature\",\n",
    "\"Phenomena-Life\":\"t:phenom:life\",\n",
    "\"Actions\":\"t:action\",\n",
    "\"Incidents\":\"t:incident\",\n",
    "\"States\":\"t:state\",\n",
    "\"Feelings\":\"t:psych\",\n",
    "\"Systems and rules\":\"t:rules\",\n",
    "\"Intellectual products\":\"t:ment\",\n",
    "\"Power\":\"t:power\",\n",
    "\"Abstract-Functional\":\"t:abstr:funct\",\n",
    "\"Abstract-Other\":\"t:abstr:other\",\n",
    "\"Shapes and figures\":\"t:physq:form\",\n",
    "\"Colors\":\"t:color\",\n",
    "\"Quantity\":\"t:quant\",\n",
    "\"People\":\"t:people\",\n",
    "\"Time\":\"t:time\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = {\n",
    "    'Noun': 'S',\n",
    "    'Verb': 'V',\n",
    "    'Adjective': 'A',\n",
    "    'Particle': 'PART',\n",
    "    'Numeral': 'NUM',\n",
    "    'Judgemental': 'AUX',\n",
    "    'Adverb': 'ADV',\n",
    "    'Demonstrative': 'PRO',\n",
    "    'Conjunction': 'CONJ',\n",
    "    'Interjection': 'INTJ',\n",
    "    'Suffix': \"SUFFIX!\",\n",
    "    'Unknown': \"Unknown\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выгрузка разметки в XML файл формата НКРЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(sents, name): # list of tuples: (sentence id, sentence in russian, list of words as dicts with text and tags)\n",
    "\n",
    "# create the file structure\n",
    "    data = ET.Element('?xml version=\"1.0\" encoding=\"utf-8\"?')\n",
    "    html = ET.SubElement(data, 'html')\n",
    "    head = ET.SubElement(html, 'head')\n",
    "    body = ET.SubElement(html, 'body')\n",
    "    for se in sents:\n",
    "        para = ET.SubElement(body, 'para')\n",
    "        para.set('id', se[0])\n",
    "        \n",
    "        rus = ET.SubElement(para, 'se')\n",
    "        rus.set('lang', 'rus')\n",
    "        rus.text = se[1]\n",
    "        \n",
    "        jap = ET.SubElement(para, 'se')\n",
    "        jap.set('lang', 'jpn')\n",
    "        \n",
    "        for word in se[2][0]: #se[2] - список словарей word с ключами text, lex и другие\n",
    "            w = ET.SubElement(jap, 'w')\n",
    "            #if word[has_translit] ==\n",
    "            if \"punct\" not in word.keys():\n",
    "                ana = ET.SubElement(w, 'ana')\n",
    "                ana.text = word['text']\n",
    "            \n",
    "                for key in word.keys():\n",
    "                    #print(key, word[key])\n",
    "                    if key != 'text' and word[key] != \"\": \n",
    "                        \n",
    "                        if key == 'has_translit' and word[key] == 'true':           # -------- CHANGES\n",
    "                            w.set('has_translit', 'true')           # -------- CHANGES\n",
    "                        else:\n",
    "                            ana.set(key, word[key])\n",
    "            else:\n",
    "                w.text = word['text']\n",
    "                    \n",
    "\n",
    "    # create a new XML file with the results\n",
    "    mydata = ET.tostring(data, encoding=\"unicode\")\n",
    "    \n",
    "    with open(name, \"w\") as f:\n",
    "        f.write(mydata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество переразмеченных слов в этом тексте: 37\n"
     ]
    }
   ],
   "source": [
    "exname = \"/Users/alinazabolotskaya/Desktop/annotated/11.xml\"\n",
    "exnew_name = \"/Users/alinazabolotskaya/Desktop/annotated/11_new.xml\"\n",
    "tree = to_tree(exname)\n",
    "data = get_tags(restructure(tree))\n",
    "to_file(data, exnew_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переразметка всего корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Количество переразмеченных слов в этом тексте: 42\n",
      "2\n",
      "3\n",
      "Количество переразмеченных слов в этом тексте: 14\n",
      "4\n",
      "Количество переразмеченных слов в этом тексте: 48\n",
      "5\n",
      "Количество переразмеченных слов в этом тексте: 43\n",
      "6\n",
      "Количество переразмеченных слов в этом тексте: 106\n",
      "7\n",
      "Количество переразмеченных слов в этом тексте: 25\n",
      "8\n",
      "Количество переразмеченных слов в этом тексте: 63\n",
      "9\n",
      "Количество переразмеченных слов в этом тексте: 33\n",
      "10\n",
      "11\n",
      "Количество переразмеченных слов в этом тексте: 37\n",
      "12\n",
      "Количество переразмеченных слов в этом тексте: 33\n",
      "13\n",
      "Количество переразмеченных слов в этом тексте: 28\n",
      "14\n",
      "Количество переразмеченных слов в этом тексте: 14\n",
      "15\n",
      "Количество переразмеченных слов в этом тексте: 35\n",
      "16\n",
      "Количество переразмеченных слов в этом тексте: 114\n",
      "17\n",
      "Количество переразмеченных слов в этом тексте: 111\n",
      "attachment_form повторяется!\n",
      "18\n",
      "Количество переразмеченных слов в этом тексте: 51\n",
      "19\n",
      "Количество переразмеченных слов в этом тексте: 77\n",
      "20\n",
      "Количество переразмеченных слов в этом тексте: 46\n",
      "21\n",
      "Количество переразмеченных слов в этом тексте: 52\n",
      "22\n",
      "Количество переразмеченных слов в этом тексте: 12\n",
      "23\n",
      "Количество переразмеченных слов в этом тексте: 67\n",
      "24\n",
      "Количество переразмеченных слов в этом тексте: 57\n",
      "25\n",
      "Количество переразмеченных слов в этом тексте: 67\n",
      "attachment_form повторяется!\n",
      "26\n",
      "Количество переразмеченных слов в этом тексте: 22\n",
      "27\n",
      "Количество переразмеченных слов в этом тексте: 82\n",
      "28\n",
      "Количество переразмеченных слов в этом тексте: 106\n",
      "29\n",
      "Количество переразмеченных слов в этом тексте: 54\n",
      "30\n",
      "Количество переразмеченных слов в этом тексте: 158\n",
      "attachment_form повторяется!\n",
      "31\n",
      "Количество переразмеченных слов в этом тексте: 162\n",
      "type повторяется!\n",
      "32\n",
      "Количество переразмеченных слов в этом тексте: 67\n",
      "33\n",
      "Количество переразмеченных слов в этом тексте: 143\n",
      "attachment_form повторяется!\n",
      "type повторяется!\n",
      "type повторяется!\n",
      "type повторяется!\n",
      "type повторяется!\n",
      "type повторяется!\n",
      "34\n",
      "Количество переразмеченных слов в этом тексте: 59\n",
      "attachment_form повторяется!\n",
      "attachment_form повторяется!\n",
      "35\n",
      "Количество переразмеченных слов в этом тексте: 92\n",
      "36\n",
      "Количество переразмеченных слов в этом тексте: 72\n",
      "37\n",
      "Количество переразмеченных слов в этом тексте: 39\n",
      "38\n",
      "Количество переразмеченных слов в этом тексте: 50\n",
      "39\n",
      "Количество переразмеченных слов в этом тексте: 32\n",
      "40\n",
      "Количество переразмеченных слов в этом тексте: 23\n",
      "41\n",
      "Количество переразмеченных слов в этом тексте: 29\n",
      "42\n",
      "Количество переразмеченных слов в этом тексте: 32\n",
      "43\n",
      "Количество переразмеченных слов в этом тексте: 24\n",
      "44\n",
      "Количество переразмеченных слов в этом тексте: 140\n",
      "type повторяется!\n",
      "type повторяется!\n",
      "attachment_form повторяется!\n",
      "attachment_form повторяется!\n",
      "45\n",
      "Количество переразмеченных слов в этом тексте: 16\n",
      "46\n",
      "Количество переразмеченных слов в этом тексте: 25\n",
      "47\n",
      "Количество переразмеченных слов в этом тексте: 19\n",
      "48\n",
      "Количество переразмеченных слов в этом тексте: 52\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 50):\n",
    "    print(i)\n",
    "    name = \"/Users/alinazabolotskaya/Desktop/annotated/\" + str(i)+\".xml\"\n",
    "    new_name = \"/Users/alinazabolotskaya/Desktop/annotated/\" + str(i)+\"_new.xml\"\n",
    "    try:\n",
    "        tree = to_tree(name)\n",
    "        data = get_tags(restructure(tree))\n",
    "        to_file(data, new_name)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переразмеченные файлы корпуса находятся в папке texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
